# Evaluation Metrics

## Evaluations in Weave
In Weave, Scorers are used to evaluate AI outputs and return evaluation metrics. They take the AI's output, analyze it, and return a dictionary of results. Scorers can use your input data as reference if needed and can also output extra information, such as explanations or reasonings from the evaluation.

Scorers are passed to a `weave.Evaluation` object during evaluation. There are two types of Scorers in weave:

1. **Function-based Scorers:** Simple Python functions decorated with `@weave.op`.
2. **Class-based Scorers:** Python classes that inherit from `weave.Scorer` for more complex evaluations.

Scorers must return a dictionary and can return multiple metrics, nested metrics and non-numeric values such as texted returned from a LLM-evaluator about its reasoning.

## Function-based Scorers
These are functions, decorated with @weave.op that return a dictionary. They're great for simple evaluations:

```python
@weave.op
def evaluate_uppercase(text: str):
    return  {"text_is_uppercase": text.isupper()}

eval = weave.Evaluations(..., scorers=[evaluate_uppercase])
```

When the evaluation is run, `evaluate_uppercase` checks if the text is all uppercase.

## Class-based Scorers
For more advanced evaluations, especially when you need to keep track of additional scorer metadata, try different prompts for your LLM-evaluators or make multiple function calls, you can use the `Scorer` class.

**Requirements:**
- Inherit from `weave.Scorer`.
- Define a `score` method decorated with `@weave.op`.
- The `score` method must return a dictionary.

Example:


```python
from weave import Scorer

class SummarizationScorer(Scorer):
    model_id: str = "the LLM model to use"
    system_prompt: str = "Evaluate whether the summary is good."

    @weave.op
    def some_complicated_preprocessing(text):
        ...
        return text

    @weave.op
    def llm_call(summary, text):
        res = create(self.system_prompt, summary, text)
        return {"summary_quality": res}

    @weave.op
    def score(output, text)
        '''
            output: The summary generated by an AI system
            text: The original text being summarised
        '''
        text = some_complicated_preprocessing(text)
        eval_result = call_llm(summary, text, self.prompt)
        return {"summary_quality": eval_result}

summarization_scorer = SummarizationScorer(model_id="o2")
eval = weave.Evaluations(..., scorers=[summarization_scorer])
```
This class evaluates how good a summary is by comparing it to the original text.

## How Scorers Work
### Keyword Arguments
Scorers can access both the output from your AI system and the input data.

- **Output:** Include an `output` parameter in your scorer function's signature to access the AI system's output.

- **Input:** Add parameters that match the names of the columns in your dataset to access input data.

For example, if your dataset has a "news_article" column, you can access it in the scorer by adding a `news_article` parameter to your scorer's signature.

### Mapping Column Names
Sometimes, the scorer's parameter names don't match the column names in your dataset. You can fix this using a `column_map`.

If you're using a class-based scorer, pass a dictionary to the `column_map` attribute of `Scorer` when you initialise your scorer class. This dictionary maps your scorer's parameter names to the dataset's column names, in the order: `{scorer keyword argument : dataset column name}`.

Example:

```python
from weave import Scorer

# A dataset with news articles to be summarised
dataset = [
    {"news_article": "The news today was great...", "date": "2030-04-20", "source": "Bright Sky Network"}
    ...
]

# Scorer class
class SummarizationScorer(Scorer)
    
    @weave.op
    def score(output, text)
        """
            output: output summary from a LLM summarization system
            text: the text being summarised
        """
        ...  # evaluate the quality of the summary

# create a scorer with a column mapping the `text` parameter to the `news_article` data column
scorer = SummarizationScorer(column_map = {"text" : "news_article"})
```
Here, the `text` parameter in the score method will receive data from the `news_article` column.


## Predefined Scorers

**LLM-evaluators**

The pre-defined scorers that use LLMs support the OpenAI, Anthropic, Google GenerativeAI and MistralAI clients. They also uses weave's `InstructorLLMScorer` class, so you'll need to install the [`instructor`](https://github.com/instructor-ai/instructor) Python package to be able to use them.

### `HallucinationFreeScorer`

This scorer checks if your AI system's output includes any hallucinations based on the input data.

```python
from weave.scorers import HallucinationFreeScorer

llm_client = # initialize your LLM client here

scorer = HallucinationFreeScorer(
    client=llm_client, 
    model_id="gpt4o"
)
```

**Customization:**
- Customize the `system_prompt` and `user_prompt` attributes of the scorer to define what "hallucination" means for you.

**Notes:**
- The `score` method expects an input column named `context`. If your dataset uses a different name, use the `column_map` attribute to map `context` to the dataset column.

---

### `SummarizationScorer`

Use an LLM to compare a summary to the original text and evaluate the quality of the summary.

```python
from weave.scorers import SummarizationScorer

llm_client = # initialize your LLM client here

scorer = SummarizationScorer(
    client=llm_client, 
    model_id="gpt4o"
)
```

**How It Works:**

This scorer evaluates summaries in two ways:

1. **Entity Density:** Checks the ratio of unique entities (like names, places, or things) mentioned in the summary to the total word count in the summary in order to estimate the "information density" of the summary. Uses an LLM to extract the entities. Similar to how entity density is used in the Chain of Density paper, https://arxiv.org/abs/2309.04269

2. **Quality Grading:** Uses an LLM-evaluator to grade the summary as `poor`, `ok`, or `excellent`. These grades are converted to scores (0.0 for poor, 0.5 for ok, and 1.0 for excellent) so you can calculate averages.

**Customization:**
- Adjust `summarization_evaluation_system_prompt` and `summarization_evaluation_prompt` to define what makes a good summary.

**Notes:**
- This scorer uses the `InstructorLLMScorer` class.
- The `score` method expects the original text that was summarized to be present in the `input` column of the dataset. Use the `column_map` class attribute to map `input` to the correct dataset column if needed.


---

### `OpenAIModerationScorer`

The `OpenAIModerationScorer` uses OpenAI's Moderation API to check if the AI system's output contains disallowed content, such as hate speech or explicit material.

```python
from weave.flow.scorers.moderation_scorer import OpenAIModerationScorer
import openai

oai_client = OpenAI(api_key=...) # initialize your LLM client here

scorer = OpenAIModerationScorer(
    client=oai_client,
    model_id="text-embedding-3-small"
)
```

**How It Works:**

- Sends the AI's output to the OpenAI Moderation endpoint and returns a dictionary indicating whether the content is flagged and details about the categories involved.

**Notes:**
- Requires the `openai` Python package.
- The client must be an instance of OpenAI's `OpenAI` or `AsyncOpenAI` client.

---

### `EmbeddingSimilarityScorer`

The `EmbeddingSimilarityScorer` computes the cosine similarity between the embeddings of the AI system's output and a target text from your dataset. It's useful for measuring how similar the AI's output is to a reference text.

```python
from weave.flow.scorers.similarity_score import EmbeddingSimilarityScorer

llm_client = ...  # initialise your LlM client

similarity_scorer = EmbeddingSimilarityScorer(
    client=llm_client
    target_column="reference_text",  # the dataset column to compare the output against
    threshold=0.4  # the cosine similarity threshold to use  
)
```

**Parameters:**

- `target_column`: Name of the dataset column containing the reference text (default is `"text"`).
- `threshold` (float): Minimum cosine similarity score considered as similar (default is `0.5`).

---

### `ValidJSONScorer`

The ValidJSONScorer checks whether the AI system's output is valid JSON. This scorer is useful when you expect the output to be in JSON format and need to verify its validity.

```python
from weave.flow.scorers.json_scorer import ValidJSONScorer

json_scorer = ValidJSONScorer()
```

**Notes:**
- If the output cannot be parsed as JSON, or if it parses to a data type other than dict or list, it is considered invalid.


---

### `ValidXMLScorer`

The `ValidXMLScorer` checks whether the AI system's output is valid XML. This is useful when expecting XML-formatted outputs.

```python
from weave.flow.scorers.xml_scorer import ValidXMLScorer

xml_scorer = ValidXMLScorer()
```

---

### `PydanticScorer`

The `PydanticScorer` validates the AI system's output against a Pydantic model to ensure it adheres to a specified schema or data structure.

```python
from weave.flow.scorers.pydantic_scorer import PydanticScorer
from pydantic import BaseModel

class FinancialReport(BaseModel):
    revenue: int
    year: str

pydantic_scorer = PydanticScorer(model=Person)
```

---

### RAGAS - `ContextEntityRecallScorer`

The `ContextEntityRecallScorer` estimates context recall by extracting entities from both the AI system's output and the provided context, then computing the recall score. Based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library

```python
from weave.flow.scorers.ragas_scorer import ContextEntityRecallScorer

llm_client = ...  # initialise your LlM client

entity_recall_scorer = ContextEntityRecallScorer(
    client=llm_client
    model_id="your-model-id"
    )
```

**How It Works:**

- Uses an LLM to extract unique entities from the output and context.
- Calculates recall as the proportion of entities in the output that are present in the context.
- Returns a dictionary with the recall score.

**Notes:**

- Expects a `context` column in your dataset, use `column_map` to map `context` to another dataset column if needed.

---

### RAGAS - `ContextRelevancyScorer`

The `ContextRelevancyScorer` evaluates the relevancy of the provided context to the AI system's output. It helps determine if the context used is appropriate for generating the output. Based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library.

```python
from weave.flow.scorers.ragas_scorer import ContextRelevancyScorer

llm_client = ...  # initialise your LlM client

relevancy_scorer = ContextRelevancyScorer(
    llm_client = ...  # initialise your LlM client
    model_id="your-model-id"
    )
```

**How It Works:**

- Uses an LLM to rate the relevancy of the context to the output on a scale from 0 to 1.
- Returns a dictionary with the `relevancy_score`.

**Notes:**

- Expects a `context` column in your dataset, use `column_map` to map `context` to another dataset column if needed.
- Customize the `relevancy_prompt` to define how relevancy is assessed.